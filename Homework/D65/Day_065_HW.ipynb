{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 作業:\n",
    "嘗試調整參數:  \n",
    "sg:sg=1表示採用skip-gram,sg=0 表示採用cbow  \n",
    "window:能往左往右看幾個字的意思 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim, logging\n",
    "from gensim.models import word2vec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 日誌訊息設定\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-23 05:52:51,710 : INFO : collecting all words and their counts\n",
      "2019-03-23 05:52:51,711 : INFO : PROGRESS: at sentence #0, processed 0 words, keeping 0 word types\n",
      "2019-03-23 05:52:51,712 : INFO : collected 3 word types from a corpus of 4 raw words and 2 sentences\n",
      "2019-03-23 05:52:51,713 : INFO : Loading a fresh vocabulary\n",
      "2019-03-23 05:52:51,714 : INFO : effective_min_count=1 retains 3 unique words (100% of original 3, drops 0)\n",
      "2019-03-23 05:52:51,714 : INFO : effective_min_count=1 leaves 4 word corpus (100% of original 4, drops 0)\n",
      "2019-03-23 05:52:51,715 : INFO : deleting the raw counts dictionary of 3 items\n",
      "2019-03-23 05:52:51,716 : INFO : sample=0.001 downsamples 3 most-common words\n",
      "2019-03-23 05:52:51,717 : INFO : downsampling leaves estimated 0 word corpus (5.7% of prior 4)\n",
      "2019-03-23 05:52:51,718 : INFO : estimated required memory for 3 words and 256 dimensions: 7644 bytes\n",
      "2019-03-23 05:52:51,719 : INFO : resetting layer weights\n",
      "2019-03-23 05:52:51,720 : INFO : training model with 4 workers on 3 vocabulary and 256 features, using sg=1 hs=0 sample=0.001 negative=5 window=8\n",
      "2019-03-23 05:52:51,724 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-23 05:52:51,726 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-23 05:52:51,726 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-23 05:52:51,727 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-23 05:52:51,728 : INFO : EPOCH - 1 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-23 05:52:51,732 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-23 05:52:51,733 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-23 05:52:51,734 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-23 05:52:51,735 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-23 05:52:51,736 : INFO : EPOCH - 2 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-23 05:52:51,742 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-23 05:52:51,743 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-23 05:52:51,744 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-23 05:52:51,746 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-23 05:52:51,747 : INFO : EPOCH - 3 : training on 4 raw words (1 effective words) took 0.0s, 173 effective words/s\n",
      "2019-03-23 05:52:51,753 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-23 05:52:51,754 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-23 05:52:51,754 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-23 05:52:51,755 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-23 05:52:51,756 : INFO : EPOCH - 4 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-23 05:52:51,761 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2019-03-23 05:52:51,763 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2019-03-23 05:52:51,764 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2019-03-23 05:52:51,765 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2019-03-23 05:52:51,765 : INFO : EPOCH - 5 : training on 4 raw words (0 effective words) took 0.0s, 0 effective words/s\n",
      "2019-03-23 05:52:51,766 : INFO : training on a 20 raw words (1 effective words) took 0.0s, 22 effective words/s\n",
      "2019-03-23 05:52:51,767 : WARNING : under 10 jobs per worker: consider setting a smaller `batch_words' for smoother alpha decay\n"
     ]
    }
   ],
   "source": [
    "sentences = [['I am a hero', 'sentence'], ['She is a teacher', 'sentence']] \n",
    "\n",
    "# train word2vec on the two sentences  \n",
    "# - size : 訓練出的詞向量會有幾維\n",
    "# - min_count : 若這個詞出現的次數 < min_count, 則該詞不被視為訓練對象\n",
    "# - window : 能往左往右看幾個字\n",
    "# - workers : 執行緒數字\n",
    "# - sg : sg=1 表⽰採⽤ skip-gram, sg=0 表⽰採⽤ cbow\n",
    "model = word2vec.Word2Vec(sentences, size=256, min_count=1, window=8, workers=4, sg=1)  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Word2Vec(vocab=3, size=256, alpha=0.025)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'I am a hero': <gensim.models.keyedvectors.Vocab at 0x1d3cea18ba8>,\n",
       " 'sentence': <gensim.models.keyedvectors.Vocab at 0x1d3ce8c10f0>,\n",
       " 'She is a teacher': <gensim.models.keyedvectors.Vocab at 0x1d3ce8c1a20>}"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(model)\n",
    "display(model.wv.vocab) # 所有訓練好的詞"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "I am a hero eq sentence = 0.019386\n",
      "She is a teacher eq sentence = -0.032129\n",
      "I am a hero eq She is a teacher = -0.032129\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:2: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  \n",
      "C:\\ProgramData\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:3: DeprecationWarning: Call to deprecated `similarity` (Method will be removed in 4.0.0, use self.wv.similarity() instead).\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    }
   ],
   "source": [
    "print('%s eq %s = %f' %('I am a hero', 'sentence', model.similarity('I am a hero','sentence')))\n",
    "print('%s eq %s = %f' %('She is a teacher', 'sentence', model.similarity('I am a hero','She is a teacher')))\n",
    "print('%s eq %s = %f' %('I am a hero', 'She is a teacher', model.similarity('I am a hero','She is a teacher')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2019-03-23 05:53:01,512 : INFO : saving Word2Vec object under mymodel_hw, separately None\n",
      "2019-03-23 05:53:01,513 : INFO : not storing attribute vectors_norm\n",
      "2019-03-23 05:53:01,514 : INFO : not storing attribute cum_table\n",
      "2019-03-23 05:53:01,521 : INFO : saved mymodel_hw\n",
      "2019-03-23 05:53:01,522 : INFO : loading Word2Vec object from mymodel_hw\n",
      "2019-03-23 05:53:01,524 : INFO : loading wv recursively from mymodel_hw.wv.* with mmap=None\n",
      "2019-03-23 05:53:01,525 : INFO : setting ignored attribute vectors_norm to None\n",
      "2019-03-23 05:53:01,525 : INFO : loading vocabulary recursively from mymodel_hw.vocabulary.* with mmap=None\n",
      "2019-03-23 05:53:01,526 : INFO : loading trainables recursively from mymodel_hw.trainables.* with mmap=None\n",
      "2019-03-23 05:53:01,527 : INFO : setting ignored attribute cum_table to None\n",
      "2019-03-23 05:53:01,528 : INFO : loaded mymodel_hw\n"
     ]
    }
   ],
   "source": [
    "# 把訓練好的 model 存檔, 要用時可再載入\n",
    "model.save('mymodel_hw')  # 儲存了一個檔案叫 mymodel_hw\n",
    "new_model = gensim.models.Word2Vec.load('mymodel_hw')  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
